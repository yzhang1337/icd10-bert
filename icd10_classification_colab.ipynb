{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICD-10 Classification with BERT - Colab GPU Training\n",
    "\n",
    "This notebook is designed to run on Google Colab with GPU acceleration for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and setup\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets scikit-learn accelerate -q\n",
    "!pip install sentencepiece -q  # Sometimes needed for certain tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix common Colab dataset loading issues\n",
    "!pip install datasets --upgrade -q\n",
    "!pip install huggingface_hub --upgrade -q\n",
    "\n",
    "# Clear any problematic cache\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear HuggingFace cache\n",
    "hf_cache = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "if os.path.exists(hf_cache):\n",
    "    print(\"Clearing HuggingFace cache...\")\n",
    "    try:\n",
    "        shutil.rmtree(hf_cache)\n",
    "        print(\"‚úÖ Cache cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cache clearing issue: {e}\")\n",
    "\n",
    "print(\"Environment prepared for dataset loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Troubleshooting Dataset Loading\n",
    "\n",
    "If you encounter dataset loading errors, try these solutions in order:\n",
    "\n",
    "1. **Run the cache clearing cell above**\n",
    "2. **Restart runtime**: Runtime ‚Üí Restart runtime (then re-run from the beginning)\n",
    "3. **Use streaming mode**: Uncomment the manual loading cell\n",
    "4. **Alternative dataset**: Switch to a different medical NLP dataset if needed\n",
    "\n",
    "**Common Error**: `NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported`\n",
    "**Solution**: Clear cache and restart runtime, then try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your training scripts to Colab\n",
    "# Method 1: Upload files directly\n",
    "from google.colab import files\n",
    "print(\"Upload train.py and evaluate.py files:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Method 2: Alternative - copy-paste the code directly into cells below\n",
    "# (Recommended if you want to modify training parameters easily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development: upload train.py and evaluate.py manually or use:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload train.py and evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Clear any cached dataset files to avoid caching issues\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
    "if os.path.exists(cache_dir):\n",
    "    print(\"Clearing dataset cache to avoid loading issues...\")\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(cache_dir)\n",
    "        print(\"‚úÖ Cache cleared\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Cache clearing failed, continuing anyway...\")\n",
    "\n",
    "# Load the dataset with cache disabled\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(\"FiscaAI/synth-ehr-icd10cm-prompt\", cache_dir=None)\n",
    "except Exception as e:\n",
    "    print(f\"First attempt failed: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    # Alternative: Load with different cache settings\n",
    "    dataset = load_dataset(\"FiscaAI/synth-ehr-icd10cm-prompt\", download_mode=\"force_redownload\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "\n",
    "# Examine structure\n",
    "print(\"\\nFirst example:\")\n",
    "first_example = dataset['train'][0]\n",
    "print(f\"Clinical Note: {first_example['user'][:300]}...\")\n",
    "print(f\"ICD Codes: {first_example['codes']}\")\n",
    "print(f\"ICD Code type: {type(first_example['codes'])}\")\n",
    "\n",
    "# Take a sample for analysis (start smaller to avoid memory issues)\n",
    "sample_size = min(5000, len(dataset['train']))\n",
    "print(f\"\\nTaking sample of {sample_size} examples for analysis...\")\n",
    "sample_data = dataset['train'].select(range(sample_size))\n",
    "print(\"‚úÖ Sample created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manual dataset loading if cache issues persist\n",
    "# This cell provides a backup method for loading the dataset\n",
    "\n",
    "def load_dataset_manual():\n",
    "    \"\"\"Backup method to load dataset if caching issues occur\"\"\"\n",
    "    print(\"Using manual dataset loading method...\")\n",
    "    \n",
    "    # Method 1: Load smaller chunks\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(\"FiscaAI/synth-ehr-icd10cm-prompt\", streaming=True)\n",
    "        \n",
    "        # Convert streaming to regular dataset with limited samples\n",
    "        train_data = []\n",
    "        for i, example in enumerate(dataset['train']):\n",
    "            if i >= 10000:  # Limit to first 10k for memory efficiency\n",
    "                break\n",
    "            train_data.append(example)\n",
    "        \n",
    "        from datasets import Dataset\n",
    "        dataset = {'train': Dataset.from_list(train_data)}\n",
    "        \n",
    "        print(f\"‚úÖ Manual loading successful! Loaded {len(dataset['train'])} examples\")\n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Manual loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment the line below if the cell above fails\n",
    "# dataset = load_dataset_manual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution on sample\n",
    "all_codes = []\n",
    "for example in sample_data:\n",
    "    codes = example['codes']\n",
    "    if isinstance(codes, str):\n",
    "        codes = [codes]\n",
    "    all_codes.extend(codes)\n",
    "\n",
    "code_counts = Counter(all_codes)\n",
    "print(f\"Total unique ICD codes in sample: {len(code_counts)}\")\n",
    "print(f\"Total code instances: {len(all_codes)}\")\n",
    "print(f\"Average codes per note: {len(all_codes) / len(sample_data):.2f}\")\n",
    "\n",
    "print(f\"\\nTop 15 most common codes:\")\n",
    "for code, count in code_counts.most_common(15):\n",
    "    print(f\"  {code}: {count} ({count/len(sample_data)*100:.1f}%)\")\n",
    "\n",
    "# Check label distribution\n",
    "multi_label_count = sum(1 for ex in sample_data \n",
    "                       if isinstance(ex['codes'], list) and len(ex['codes']) > 1)\n",
    "print(f\"\\nMulti-label examples: {multi_label_count} ({multi_label_count/len(sample_data)*100:.1f}%)\")\n",
    "\n",
    "# Text length analysis\n",
    "text_lengths = [len(ex['user'].split()) for ex in sample_data]\n",
    "print(f\"\\nText length stats:\")\n",
    "print(f\"  Mean: {np.mean(text_lengths):.1f} words\")\n",
    "print(f\"  Median: {np.median(text_lengths):.1f} words\")\n",
    "print(f\"  Max: {np.max(text_lengths)} words\")\n",
    "print(f\"  95th percentile: {np.percentile(text_lengths, 95):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Optimized Training - Recommended Settings\n",
    "# A100 GPU can handle much larger batches and longer sequences\n",
    "\n",
    "print(\"Starting GPU-optimized training...\")\n",
    "print(\"This will take 15-30 minutes depending on sample size\")\n",
    "\n",
    "!python train.py \\\n",
    "    --model_name \"emilyalsentzer/Bio_ClinicalBERT\" \\\n",
    "    --batch_size 32 \\\n",
    "    --epochs 8 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --max_length 512 \\\n",
    "    --sample_size 15000 \\\n",
    "    --fp16 \\\n",
    "    --output_dir ./model_gpu_trained\n",
    "\n",
    "print(\"\\nTraining completed! Model saved to ./model_gpu_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Quick Training for Testing (5-10 minutes)\n",
    "# Use this for rapid iteration and testing\n",
    "\n",
    "print(\"Quick training for testing...\")\n",
    "\n",
    "!python train.py \\\n",
    "    --model_name \"emilyalsentzer/Bio_ClinicalBERT\" \\\n",
    "    --batch_size 64 \\\n",
    "    --epochs 3 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --fp16 \\\n",
    "    --sample_size 5000 \\\n",
    "    --output_dir ./model_quick_test\n",
    "\n",
    "print(\"\\nQuick training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Evaluation on GPU-trained model\n",
    "print(\"Running evaluation on GPU-trained model...\")\n",
    "\n",
    "!python evaluate.py --model_dir ./model_gpu_trained --sample_size 2000\n",
    "\n",
    "# Also evaluate the quick test model for comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Comparing with quick test model...\")\n",
    "!python evaluate.py --model_dir ./model_quick_test --sample_size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare results from both models\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load GPU-trained model results\n",
    "try:\n",
    "    with open('./model_gpu_trained/evaluation_results.json', 'r') as f:\n",
    "        gpu_results = json.load(f)\n",
    "    print(\"GPU-Trained Model Results:\")\n",
    "    print(f\"  Micro F1: {gpu_results['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1: {gpu_results['macro_f1']:.4f}\")\n",
    "    print(\"  Top-K Accuracy:\")\n",
    "    for k, acc in gpu_results['top_k_accuracy'].items():\n",
    "        print(f\"    Top-{k}: {acc:.4f}\")\n",
    "except:\n",
    "    print(\"GPU model results not found\")\n",
    "\n",
    "# Load quick test model results\n",
    "try:\n",
    "    with open('./model_quick_test/evaluation_results.json', 'r') as f:\n",
    "        quick_results = json.load(f)\n",
    "    print(\"\\nQuick Test Model Results:\")\n",
    "    print(f\"  Micro F1: {quick_results['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1: {quick_results['macro_f1']:.4f}\")\n",
    "    print(\"  Top-K Accuracy:\")\n",
    "    for k, acc in quick_results['top_k_accuracy'].items():\n",
    "        print(f\"    Top-{k}: {acc:.4f}\")\n",
    "except:\n",
    "    print(\"Quick test model results not found\")\n",
    "\n",
    "# Create comparison visualization\n",
    "if 'gpu_results' in locals() and 'quick_results' in locals():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # F1 Scores comparison\n",
    "    models = ['GPU Trained', 'Quick Test']\n",
    "    micro_f1s = [gpu_results['micro_f1'], quick_results['micro_f1']]\n",
    "    macro_f1s = [gpu_results['macro_f1'], quick_results['macro_f1']]\n",
    "    \n",
    "    x = range(len(models))\n",
    "    ax1.bar([i-0.2 for i in x], micro_f1s, 0.4, label='Micro F1', alpha=0.7)\n",
    "    ax1.bar([i+0.2 for i in x], macro_f1s, 0.4, label='Macro F1', alpha=0.7)\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_title('F1 Score Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Top-K Accuracy comparison\n",
    "    k_values = [1, 5, 10]\n",
    "    gpu_topk = [gpu_results['top_k_accuracy'][str(k)] for k in k_values]\n",
    "    quick_topk = [quick_results['top_k_accuracy'][str(k)] for k in k_values]\n",
    "    \n",
    "    ax2.plot(k_values, gpu_topk, 'o-', label='GPU Trained', linewidth=2)\n",
    "    ax2.plot(k_values, quick_topk, 's-', label='Quick Test', linewidth=2)\n",
    "    ax2.set_xlabel('K (Top-K)')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Top-K Accuracy Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Experimentation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ OPTIONAL: Model Architecture Experiments\n",
    "# Try different BERT variants to see which performs best\n",
    "\n",
    "models_to_try = [\n",
    "    (\"Clinical BERT\", \"emilyalsentzer/Bio_ClinicalBERT\"),\n",
    "    (\"PubMed BERT\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"),\n",
    "    (\"General BERT\", \"bert-base-uncased\"),\n",
    "    (\"DistilBERT\", \"distilbert-base-uncased\")\n",
    "]\n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "print(\"üß™ Running model comparison experiments...\")\n",
    "print(\"This will take 30-45 minutes total\")\n",
    "\n",
    "for model_name, model_path in models_to_try:\n",
    "    print(f\"\\nüîÑ Training {model_name}...\")\n",
    "    output_dir = f\"./experiments/{model_name.lower().replace(' ', '_')}\"\n",
    "    \n",
    "    # Train each model with same settings\n",
    "    !python train.py \\\n",
    "        --model_name {model_path} \\\n",
    "        --batch_size 64 \\\n",
    "        --epochs 2 \\\n",
    "        --learning_rate 2e-5 \\\n",
    "        --max_length 256 \\\n",
    "        --fp16 \\\n",
    "        --sample_size 3000 \\\n",
    "        --output_dir {output_dir}\n",
    "    \n",
    "    # Evaluate\n",
    "    !python evaluate.py --model_dir {output_dir} --sample_size 500\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} completed\")\n",
    "\n",
    "print(\"\\nüèÜ Experiment completed! Check individual results in ./experiments/\")\n",
    "\n",
    "# Compare all results\n",
    "print(\"\\nüìä COMPARISON SUMMARY:\")\n",
    "for model_name, _ in models_to_try:\n",
    "    dir_name = f\"./experiments/{model_name.lower().replace(' ', '_')}\"\n",
    "    try:\n",
    "        with open(f'{dir_name}/evaluation_results.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"{model_name:15} - Micro F1: {results['micro_f1']:.4f}, Top-5: {results['top_k_accuracy']['5']:.4f}\")\n",
    "    except:\n",
    "        print(f\"{model_name:15} - Results not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ EXPORT TRAINED MODEL FOR LOCAL USE \n",
    "\n",
    "print(\"üöÄ Preparing model for download...\")\n",
    "\n",
    "# Zip the main GPU-trained model\n",
    "!zip -r gpu_trained_model.zip model_gpu_trained/\n",
    "print(\"‚úÖ GPU-trained model zipped\")\n",
    "\n",
    "# Also zip the quick test model for comparison\n",
    "!zip -r quick_test_model.zip model_quick_test/\n",
    "print(\"‚úÖ Quick test model zipped\")\n",
    "\n",
    "# Check file sizes\n",
    "!ls -lh *.zip\n",
    "\n",
    "print(\"\\nüìã INSTRUCTIONS FOR LOCAL USE:\")\n",
    "print(\"1. Download both zip files to your local machine\")\n",
    "print(\"2. Extract: unzip gpu_trained_model.zip\")\n",
    "print(\"3. Use locally: python evaluate.py --model_dir ./model_gpu_trained\")\n",
    "print(\"4. The model will work on CPU for inference (much faster than training)\")\n",
    "\n",
    "# Download files\n",
    "from google.colab import files\n",
    "print(\"\\n‚¨áÔ∏è Downloading GPU-trained model...\")\n",
    "files.download('gpu_trained_model.zip')\n",
    "\n",
    "print(\"‚¨áÔ∏è Downloading quick test model...\")\n",
    "files.download('quick_test_model.zip')\n",
    "\n",
    "print(\"\\nüéâ Models ready for local deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start Guide for Colab\n",
    "\n",
    "**RECOMMENDED WORKFLOW:**\n",
    "\n",
    "1. **Upload Scripts**: Run cell above to upload `train.py` and `evaluate.py`\n",
    "2. **Quick Test**: Run the \"Quick Training\" cell first (5-10 minutes)\n",
    "3. **Full Training**: If satisfied, run the \"GPU-Optimized Training\" (15-30 minutes)\n",
    "4. **Download Model**: Use the export cell to download trained weights\n",
    "5. **Local Deployment**: Extract and use the model locally for inference\n",
    "\n",
    "**EXPECTED PERFORMANCE:**\n",
    "- Quick Test: Micro F1 ~0.01-0.05, Top-5 Accuracy ~0.05-0.15\n",
    "- Full GPU Training: Micro F1 ~0.05-0.15, Top-5 Accuracy ~0.15-0.35\n",
    "- Much better than local CPU training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
