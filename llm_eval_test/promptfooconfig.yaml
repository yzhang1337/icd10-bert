# Promptfoo Configuration for ICD-10 Model Comparison
# Compare fine-tuned Bio_ClinicalBERT against LLM prompt approaches

description: "ICD-10 diagnosis code extraction: Fine-tuned model vs Prompt-based LLMs"

providers:
  # Fine-tuned Bio_ClinicalBERT model
  - id: finetune-bio-clinical-bert
    label: "Fine-tuned Bio_ClinicalBERT"
    config:
      provider: "file://providers/finetune_provider.py"
      model_dir: "../model_output_demo"
      max_length: 512
      threshold: 0.5

  # OpenAI GPT-4 Turbo
  - id: gpt-4-turbo
    label: "GPT-4 Turbo"
    config:
      provider: "openai:gpt-4-turbo-preview"
      temperature: 0.1
      max_tokens: 1000

  # OpenAI GPT-3.5 Turbo (cost comparison)
  - id: gpt-3.5-turbo
    label: "GPT-3.5 Turbo"
    config:
      provider: "openai:gpt-3.5-turbo"
      temperature: 0.1
      max_tokens: 1000

  # Anthropic Claude 3 Sonnet
  - id: claude-3-sonnet
    label: "Claude 3 Sonnet"
    config:
      provider: "anthropic:claude-3-sonnet-20240229"
      temperature: 0.1
      max_tokens: 1000

  # Anthropic Claude 3 Haiku (cost comparison)
  - id: claude-3-haiku
    label: "Claude 3 Haiku"
    config:
      provider: "anthropic:claude-3-haiku-20240307"
      temperature: 0.1
      max_tokens: 1000

prompts:
  # Basic direct extraction prompt
  - id: basic_extraction
    label: "Basic ICD Extraction"
    content: "file://prompts/basic_extraction.txt"

  # Few-shot learning with medical examples
  - id: few_shot
    label: "Few-shot Examples"
    content: "file://prompts/few_shot.txt"

  # Chain of thought reasoning
  - id: chain_of_thought
    label: "Chain of Thought"
    content: "file://prompts/chain_of_thought.txt"

  # Structured JSON output
  - id: structured_output
    label: "Structured Output"
    content: "file://prompts/structured_output.txt"

tests:
  # Test cases from SynthEHR dataset
  - vars:
      clinical_note: "{{clinical_note}}"
    assert:
      - type: "file://evaluators/icd_metrics.py"
        value: "{{expected_codes}}"

# Custom evaluators for medical metrics
evaluators:
  - id: icd_f1_micro
    type: "file://evaluators/icd_metrics.py"
    config:
      metric: "micro_f1"
      
  - id: icd_f1_macro
    type: "file://evaluators/icd_metrics.py"
    config:
      metric: "macro_f1"
      
  - id: icd_top_k
    type: "file://evaluators/icd_metrics.py"
    config:
      metric: "top_k_accuracy"
      k_values: [1, 5, 10]

# Output configuration
outputPath: "./results"

# Test dataset configuration
dataset:
  - file: "datasets/test_cases.yaml"

# Global settings
defaultTest:
  options:
    transform: "file://scripts/preprocess_clinical_note.js"

# Caching for expensive LLM calls
cache:
  enabled: true
  type: "disk"
  path: ".promptfoo-cache"

# Cost tracking
cost:
  enabled: true
  track:
    - provider: "openai:*"
    - provider: "anthropic:*"